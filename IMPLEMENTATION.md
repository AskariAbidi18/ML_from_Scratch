# BareMetalML Implementation Guide

This document provides a **deep dive** into the **mathematical foundations** and **practical implementation** of BareMetalML components.  
It is intended for **learning, experimentation, and educational purposes**.

All classes are **modular**, so you can import them directly:

```python
from baremetalml import LinearRegression, StandardScaler, KNNClassifier
```

## 1. Base Classes

### 1.1 BaseModel

**Purpose**: Abstract class for all models with common interfaces and input validation.

**Responsibilities**:

*fit(X, y)* â€“ Train the model

*predict(X)* â€“ Make predictions

**Input validation**: *check_x_y* and *check_x*

**Code Snippet**:

```python
class BaseModel:
    def fit(self, X, y):
        raise NotImplementedError
    def predict(self, X):
        raise NotImplementedError

```

**Why it matters**: Ensures consistency and reduces repetitive code across models.

### 1.2 BaseTransformer

**Purpose**: Abstract class for all data transformers.

**Methods**:

*fit(X)* â€“ Learn parameters from data

*transform(X)* â€“ Apply transformation

*fit_transform(X)* â€“ Combines fit + transform

**Code Snippet**:

```python
class BaseTransformer:
    def fit(self, X):
        raise NotImplementedError
    def transform(self, X):
        raise NotImplementedError
    def fit_transform(self, X):
        self.fit(X)
        return self.transform(X)
```

## 2. Linear Regression

### 2.1 Mathematical Formulation

**Linear regression predicts**:

ð‘¦
^
=
ð‘‹
ð›½
+  
ðœ–
y
^
â€‹
 =XÎ²+Ïµ

*Where*:

ð‘‹
âˆˆ
ð‘…
ð‘›
Ã—
ð‘‘
XâˆˆR 
nÃ—d
  = input matrix

ð›½
âˆˆ
ð‘…
ð‘‘
Î²âˆˆR 
d
  = weights

ðœ–
Ïµ = error

**Mean Squared Error (MSE)**:

*MSE*
=
1
ð‘›
âˆ‘
ð‘–
=
1
ð‘›
(
ð‘¦
ð‘–
âˆ’
ð‘¦
^
ð‘–
)
2
MSE= 
n
1
â€‹
  
i=1
âˆ‘
n
â€‹
 (y 
i
â€‹
 âˆ’ 
y
^
â€‹
  
i
â€‹
 ) 
2
 
**Normal Equation (Analytical solution):**

ð›½
^
=
(
ð‘‹
ð‘‡
ð‘‹
)
âˆ’
1
ð‘‹
ð‘‡
ð‘¦
Î²
^
â€‹
 =(X 
T
 X) 
âˆ’1
 X 
T
 y
Gradient Descent (Iterative solution):

ð›½
:
=
ð›½
âˆ’
ð›¼
1
ð‘›
ð‘‹
ð‘‡
(
ð‘‹
ð›½
âˆ’
ð‘¦
)
Î²:=Î²âˆ’Î± 
n
1
â€‹
 X 
T
 (XÎ²âˆ’y)
Where 
ð›¼
Î± = learning rate.

2.2 Implementation in BareMetalML
python
Copy code
lr = LinearRegression(method="gradient_descent", learning_rate=0.01, n_iterations=1000)
lr.fit(X, y)
y_pred = lr.predict(X)
Highlights:

Supports Normal Equation & Gradient Descent

Automatically handles bias/intercept

Computes predictions as:

ð‘¦
^
=
ð‘‹
â‹…
weights
+
bias
y
^
â€‹
 =Xâ‹…weights+bias
3. Logistic Regression
3.1 Mathematical Formulation
Sigmoid function:

ðœŽ
(
ð‘§
)
=
1
1
+
ð‘’
âˆ’
ð‘§
Ïƒ(z)= 
1+e 
âˆ’z
 
1
â€‹
 
Prediction:

ð‘¦
^
=
ðœŽ
(
ð‘‹
ð›½
)
y
^
â€‹
 =Ïƒ(XÎ²)
Binary Cross-Entropy Loss:

ð¿
(
ð›½
)
=
âˆ’
1
ð‘›
âˆ‘
ð‘–
=
1
ð‘›
[
ð‘¦
ð‘–
log
â¡
(
ð‘¦
^
ð‘–
)
+
(
1
âˆ’
ð‘¦
ð‘–
)
log
â¡
(
1
âˆ’
ð‘¦
^
ð‘–
)
]
L(Î²)=âˆ’ 
n
1
â€‹
  
i=1
âˆ‘
n
â€‹
 [y 
i
â€‹
 log( 
y
^
â€‹
  
i
â€‹
 )+(1âˆ’y 
i
â€‹
 )log(1âˆ’ 
y
^
â€‹
  
i
â€‹
 )]
Gradient Descent Updates:

ð›½
:
=
ð›½
âˆ’
ð›¼
1
ð‘›
ð‘‹
ð‘‡
(
ð‘¦
^
âˆ’
ð‘¦
)
Î²:=Î²âˆ’Î± 
n
1
â€‹
 X 
T
 ( 
y
^
â€‹
 âˆ’y)
3.2 Implementation
python
Copy code
logr = LogisticRegression(n_iterations=1000, learning_rate=0.01)
logr.fit(X, y)
y_pred = logr.predict(X)
Computes probabilities using sigmoid

Updates weights via gradient of cross-entropy loss

Predicts 0/1 based on 0.5 threshold

4. K-Nearest Neighbors (KNN)
4.1 Mathematical Formulation
Distance Metrics:

Euclidean: 
ð‘‘
=
âˆ‘
(
ð‘¥
ð‘–
âˆ’
ð‘¥
ð‘—
)
2
d= 
âˆ‘(x 
i
â€‹
 âˆ’x 
j
â€‹
 ) 
2
 
â€‹
 

Manhattan: 
ð‘‘
=
âˆ‘
âˆ£
ð‘¥
ð‘–
âˆ’
ð‘¥
ð‘—
âˆ£
d=âˆ‘âˆ£x 
i
â€‹
 âˆ’x 
j
â€‹
 âˆ£

Minkowski: 
ð‘‘
=
(
âˆ‘
âˆ£
ð‘¥
ð‘–
âˆ’
ð‘¥
ð‘—
âˆ£
ð‘
)
1
/
ð‘
d=(âˆ‘âˆ£x 
i
â€‹
 âˆ’x 
j
â€‹
 âˆ£ 
p
 ) 
1/p
 

Prediction Rules:

Classification: majority vote of k nearest neighbors

Regression: mean of k nearest neighbors

4.2 Implementation
python
Copy code
knn = KNNClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
Pipeline Illustration:

rust
Copy code
X_test -> compute distances -> select k nearest neighbors -> predict majority class
5. Transformers
5.1 StandardScaler
Equation:

ð‘‹
ð‘ 
ð‘
ð‘Ž
ð‘™
ð‘’
ð‘‘
=
ð‘‹
âˆ’
ðœ‡
ðœŽ
X 
scaled
â€‹
 = 
Ïƒ
Xâˆ’Î¼
â€‹
 
5.2 NormalScaler
Equation:

ð‘‹
ð‘›
ð‘œ
ð‘Ÿ
ð‘š
=
ð‘‹
âˆ’
ð‘‹
ð‘š
ð‘–
ð‘›
ð‘‹
ð‘š
ð‘Ž
ð‘¥
âˆ’
ð‘‹
ð‘š
ð‘–
ð‘›
X 
norm
â€‹
 = 
X 
max
â€‹
 âˆ’X 
min
â€‹
 
Xâˆ’X 
min
â€‹
 
â€‹
 
5.3 LabelEncoder
Maps categorical labels to integers.
Example: {'cat':0, 'dog':1, 'bird':2}

5.4 OneHotEncoder
Converts categories to one-hot vectors.
Example:

[
â€²
ð‘Ÿ
ð‘’
ð‘‘
â€²
,
â€²
ð‘
ð‘™
ð‘¢
ð‘’
â€²
]
â†’
[
1
0
0
1
]
[ 
â€²
 red 
â€²
 , 
â€²
 blue 
â€²
 ]â†’[ 
1
0
â€‹
  
0
1
â€‹
 ]
5.5 PolynomialFeatures
Generates all polynomial combinations up to degree 
ð‘‘
d:

(
ð‘¥
1
,
ð‘¥
2
)
â†’
(
1
,
ð‘¥
1
,
ð‘¥
2
,
ð‘¥
1
2
,
ð‘¥
1
ð‘¥
2
,
ð‘¥
2
2
)
(x 
1
â€‹
 ,x 
2
â€‹
 )â†’(1,x 
1
â€‹
 ,x 
2
â€‹
 ,x 
1
2
â€‹
 ,x 
1
â€‹
 x 
2
â€‹
 ,x 
2
2
â€‹
 )
python
Copy code
poly = PolynomialFeatures(degree=2, include_bias=True)
X_poly = poly.fit_transform(X)
6. Example Pipeline
python
Copy code
from baremetalml import StandardScaler, PolynomialFeatures, LinearRegression
import numpy as np

X = np.array([[1,2],[2,3],[3,4]])
y = np.array([3,5,7])

# Step 1: Standardize
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 2: Polynomial features
poly = PolynomialFeatures(degree=2, include_bias=True)
X_poly = poly.fit_transform(X_scaled)

# Step 3: Linear Regression
lr = LinearRegression(method='normal_equation')
lr.fit(X_poly, y)
y_pred = lr.predict(X_poly)
print("Predictions:", y_pred)
Pipeline Overview:

rust
Copy code
Raw Data -> Scaling -> Polynomial Feature Expansion -> Linear Regression -> Predictions
Notes
All models and transformers are pure NumPy, easy to inspect and extend

Designed for learning, experimentation, and building pipelines from scratch

Modular imports make it simple to use any component:

python
Copy code
from baremetalml import LinearRegression, StandardScaler, KNNClassifier